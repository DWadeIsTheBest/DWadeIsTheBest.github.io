<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[fatman]]></title>
    <url>%2F2019%2F05%2F15%2Ffatman%2F</url>
    <content type="text"><![CDATA[我是肥仔1void main() =&gt; (MyApp()) More info: 百度]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>UI</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F05%2F15%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python爬取电影天堂]]></title>
    <url>%2F2018%2F10%2F25%2FPython%E7%88%AC%E5%8F%96%E7%94%B5%E5%BD%B1%E5%A4%A9%E5%A0%82%2F</url>
    <content type="text"><![CDATA[前言： 本文非常浅显易懂，可以说是零基础也可快速掌握。如有疑问，欢迎留言，笔者会第一时间回复。本文代码存于github 一、爬虫的重要性： 如果把互联网比喻成一个蜘蛛网，那么Spider就是在网上爬来爬去的蜘蛛。网络蜘蛛通过网页的链接地址来寻找网页，从网站某一个页面（通常是首页）开始，读取网页的内容，找到在网页中的其它链接地址，然后通过这些链接地址寻找下一个网页，一直循环下去，直到把整个网站所有的网页都抓取完为止。 摘取部分网友的回复：1、之前在北京买房，谁想房价开始疯长，链家的房价等数据分析只给了一小部分，远远不能满足自己的需求。于是晚上花了几个小时的时间写了个爬虫，爬下了北京所有的小区信息及北京所有小区的所有历史成交记录。 2、我的爱人是某网络公司的销售，需要收集各种企业信息然后打电话联系。于是乎利用采集脚本抓一坨一坨的资料给她用，而她的同事天天自己搜资料整理到半夜。 二、实践：爬取电影天堂电影详情页1、网页分析及爬取第一页的详情页url从电影天堂最新电影界面。可以看到其第一页url为 http://www.ygdy8.net/html/gndy/dyzz/list_23_1.html第二页为http://www.ygdy8.net/html/gndy/dyzz/list_23_2.html，第三第四页也类似 12345678910111213141516from lxml import etreeimport requestsurl = &apos;http://www.ygdy8.net/html/gndy/dyzz/list_23_1.html&apos;headers = &#123; &apos;User_Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,&#125;response = requests.get(url,headers=headers)# response.text 是系统自己默认判断。但很遗憾判断错误，导致乱码出现。我们可以采取另外方式 response.content。自己指定格式解码# print(response.text)# print(response.content.decode(&apos;gbk&apos;))print(response.content.decode(encoding=&quot;gbk&quot;, errors=&quot;ignore&quot;)) 先以第一页为例，打印数据如下： 分析电影天堂 html 源代码，可以得出每个 table 标签就是一个电影 通过 xpath 拿到每个电影的详情url1234html = etree.HTML(text)detail_urls = html.xpath(&quot;//table[@class=&apos;tbspan&apos;]//a/@href&quot;)for detail_url in detail_urls: print(detail_url) #加上域名即为详情 url 结果如下： 2、整理代码并爬取前7页的电影列表url12345678910111213141516171819from lxml import etreeimport requests# 域名BASE_DOMAIN = &apos;http://www.ygdy8.net&apos;# url = &apos;http://www.ygdy8.net/html/gndy/dyzz/list_23_1.html&apos;HEADERS = &#123; &apos;User_Agent&apos;:&apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36&apos;,&#125;def spider(): base_url = &apos;http://www.ygdy8.net/html/gndy/dyzz/list_23_&#123;&#125;.html&apos; for x in range(1,8): url = base_url.format(x) print(url) # 求出每一页电影列表的url eg: http://www.ygdy8.net/html/gndy/dyzz/list_23_1.htmlif __name__ == &apos;__main__&apos;: spider() 3、爬取每一部电影的详情页地址1234567891011121314151617181920212223def get_detail_urls(url): response = requests.get(url, headers=HEADERS) # response.text 是系统自己默认判断。但很遗憾判断错误，导致乱码出现。我们可以采取另外方式 response.content。自己指定格式解码 # print(response.text) # print(response.content.decode(&apos;gbk&apos;)) # print(response.content.decode(encoding=&quot;gbk&quot;, errors=&quot;ignore&quot;)) text = response.content.decode(encoding=&quot;gbk&quot;, errors=&quot;ignore&quot;) # 通过 xpath 拿到每个电影的详情url html = etree.HTML(text) detail_urls = html.xpath(&quot;//table[@class=&apos;tbspan&apos;]//a/@href&quot;) detail_urls = map(lambda url:BASE_DOMAIN+url,detail_urls) #这句意思相当于下面一段代码:替换列表中的每一个url # def abc(url): # return BASE_DOMAIN+url # index = 1 # for detail_url in detail_urls: # detail_url = abc(detail_url) # detail_urls[index] = detail_url # index+1 return detail_urls 4、抓取电影详情页的数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283# 解析详情页面def parse_detail_page(url): movie = &#123;&#125; response = requests.get(url,headers = HEADERS) text = response.content.decode(&apos;gbk&apos;, errors=&apos;ignore&apos;) html = etree.HTML(text) # title = html.xpath(&quot;//div[@class=&apos;title_all&apos;]//font[@color=&apos;#07519a&apos;]&quot;) # 本行47行，下面已修改 # 打印出 [&lt;Element font at 0x10cb422c8&gt;, &lt;Element font at 0x10cb42308&gt;] # print(title) # 为了显示，我们需要转一下编码 # for x in title: # print(etree.tostring(x,encoding=&apos;utf-8&apos;).decode(&apos;utf-8&apos;)) # 我们是为了取得文字，所以修改47行 title = html.xpath(&quot;//div[@class=&apos;title_all&apos;]//font[@color=&apos;#07519a&apos;]/text()&quot;)[0] movie[&apos;title&apos;] = title zoomE = html.xpath(&quot;//div[@id=&apos;Zoom&apos;]&quot;) [0] # 求出共同的顶级容器，方便后面求职 imgs = zoomE.xpath(&quot;.//img/@src&quot;) # 求出海报和截图 cover = imgs[0] if len(imgs) &gt; 1: screenshot = imgs[1] movie[&apos;screenshot&apos;] = screenshot # print(cover) movie[&apos;cover&apos;] = cover infos = zoomE.xpath(&quot;.//text()&quot;) for index,info in enumerate(infos): if info.startswith(&apos;◎年 代&apos;): info = info.replace(&quot;◎年 代&quot;, &quot;&quot;).strip() # strip 去掉空格 movie[&apos;year&apos;] = info elif info.startswith(&quot;◎产 地&quot;): info = info.replace(&quot;◎产 地&quot;, &quot;&quot;).strip() movie[&quot;country&quot;] = info elif info.startswith(&quot;◎类 别&quot;): info = info.replace(&quot;◎类 别&quot;, &quot;&quot;).strip() movie[&quot;category&quot;] = info elif info.startswith(&quot;◎豆瓣评分&quot;): info = info.replace(&quot;◎豆瓣评分&quot;, &quot;&quot;).strip() movie[&quot;douban_rating&quot;] = info elif info.startswith(&quot;◎片 长&quot;): info = info.replace(&quot;◎片 长&quot;,&quot;&quot;).strip() movie[&quot;duration&quot;] = info elif info.startswith(&quot;◎导 演&quot;): info = info.replace(&quot;◎导 演&quot;, &quot;&quot;).strip() movie[&quot;director&quot;] = info elif info.startswith(&quot;◎主 演&quot;): actors = [] actor = info.replace(&quot;◎主 演&quot;, &quot;&quot;).strip() actors.append(actor) # 因为主演有很多个，再加上其在电影天堂中元素的特殊性，需要遍历一遍，在分别求出每一个演员 for x in range(index+1,len(infos)): # 从演员 infos 开始遍历，求出每一个演员 actor = infos[x].strip() if actor.startswith(&quot;◎&quot;): # 也就是到了标签 的 ◎ 就退出 break actors.append(actor) movie[&apos;actor&apos;] = actors elif info.startswith(&apos;◎简 介 &apos;): # info = info.replace(&apos;◎简 介 &apos;,&quot;&quot;).strip() for x in range(index+1,len(infos)): if infos[x].startswith(&quot;◎获奖情况&quot;): break profile = infos[x].strip() movie[&apos;profile&apos;] = profile # print(movie) elif info.startswith(&apos;◎获奖情况 &apos;): awards = [] # info = info.replace(&quot;◎获奖情况 &quot;, &quot;&quot;).strip() for x in range(index+1,len(infos)): if infos[x].startswith(&quot;【下载地址】&quot;): break award = infos[x].strip() awards.append(award) movie[&apos;awards&apos;] = awards # print(awards) download_url = html.xpath(&quot;//td[@bgcolor=&apos;#fdfddf&apos;]/a/@href&quot;)[0] movie[&apos;download_url&apos;] = download_url return movie 上述代码爬取了电影的每一个数据。为了让读者方便对照格式，笔者已经下载了写此篇文章时的html—— “movie.html”，放于github 中 最后结果：]]></content>
      <categories>
        <category>Python</category>
      </categories>
  </entry>
</search>
